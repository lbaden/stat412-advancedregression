---
  title: "Stats 412 Problem Set 3"
  author: "Lucy Baden"
  output: html_document
  highlight: pygments
---


  
### Music Genre
  This data set was published as a contest data set on the TunedIT web site (http://tunedit.org/challenge/music-retrieval/genres). In this competition, the objective was to develop a predictive model for classifying music into six categories. In total, there were 12,495 music samples for which 191 characteristics were determined. All predictors were continuous; many were highly correlated and the predictors spanned different scales of measurement. This data collection was created using 60 performers from which 15–20 pieces of music were selected for each performer. Then 20 segments of each piece were parameterized in order to create the final data set. Hence, the samples are inherently not independent of each other.

```{r}
library(readr)
genres <- read_csv("https://raw.githubusercontent.com/natelangholz/stat412-advancedregression/master/week-7/problem-set-3/genresTrain.csv")
```

### 1
**Random Forest**: Fit a random forest model using both CART trees and conditional inference trees to the music genre predictors. What are the differences in the models? Do you have any difficulty on the full data set? 

Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. 

```{r Ex1a}
genres$GENRE <- as.factor(genres$GENRE)
library(party)
library(randomForest)

set.seed(1)
N <- nrow(genres)
id_test <- sample(1:N, 0.25*N)
train <- genres[-id_test,]
test <- genres[id_test,]

# CART trees
RF_cart <- randomForest(GENRE ~ ., data=train, mtry=2)

# conditional inference trees
RF_cit <- cforest(GENRE ~ ., data=train, control = cforest_unbiased(mtry=2))

```

We used the randomForest function to fit a model using CART trees, and the party package's cforest function to fit a model using conditional inference trees. The full data set is large enough that the random forest is fairly slow to fit for both models, depending on the parameters chosen such as the number of trees. Thus, we fit a subset set of the full data set instead. This also allows us to compare the models' performance on the test set below.

```{r Ex2}
library(caret)
preds_cart <- predict(RF_cart, test)
preds_cit <- predict(RF_cit, train, type="response", OOB=T)

confusionMatrix(preds_cart, test$GENRE)
confusionMatrix(preds_cit, train$GENRE)

```

We can compare the models by seeing how well they predict the data. The CART random forest model gives an accuracy of 91.58%, while the conditional inference tree random forest model has an accuracy of 88.74%. 



### 2
**Data Splitting**: What data splitting method(s) would you use for these data? Explain.

We could use repeated 5-fold cross-validation for these data. Since the data are fairly large, computational efficiency is important, so we would prefer not to repeat the cross-validation too many times, or use too many folds. Random forests also already exhibits low variance from averaging across many single tree learners.


### 3
**Variable Importance**: Create a variable importance plot from your best model. What features are most important?

```{r Ex3}
varImpPlot(RF_cart)
```

The best model was the CART random forest. The most important features are PAR_SEMV24, PAR_SEM24, and PAR_SEM22.

  * * *
  
### Simulated data
Friedman introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

$$y = 10\sin(πx_1x_2)+20(x_3 −0.5)^2 +10x_4 +5x_5 +N(0,σ^2)$$
where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package mlbench contains a function called `mlbench.friedman1` that simulates these data:
```{r rf}
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```
  
### 4
**Random Forest and Variable Importance**:  Fit a random forest model to all of the predictors, then estimate the variable importance scores. Did the random forest model significantly use the uninformative predictors (V6 – V10)?

```{r Ex4}
RF_sim <- cforest(y ~ ., data=simulated, control = cforest_unbiased(mtry=2))
varimp(RF_sim)

```

The uninformative predictors V6 through V10 were not used in the model, with very small negative importance scores.


### 5
**Correlated Predictor**: Now add an additional predictor that is highly correlated with one of the informative predictors. For example:

```{r cor}
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
```

Fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?

```{r Ex5}

RF_sim_2 <- cforest(y ~ ., data=simulated, control = cforest_unbiased(mtry=2))
varimp(RF_sim_2)

simulated$duplicate2 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate2, simulated$V1)

RF_sim_3 <- cforest(y ~ ., data=simulated, control = cforest_unbiased(mtry=2))
varimp(RF_sim_3, conditional=F)

```

The importance score for V1 decreases from 6.32 to 4.70 with the addition of another highly correlated predictor. The new predictor also has a fairly high importance score of 3.42, bigger than all other variables except V1 and V2.

If we add another predictor that is also highly correlated with V1, and thus the first duplicate as well, the first highly correlated predictor has a reduced importance in the model (2.60), which is close to that of the second, new correlated predictor (2.77); however, the importance score for V1 did not change much at 3.55.

### 6
**Gradient Boosted Machine**: Repeat the process in 5 and 6 with different tree models, such as boosted trees. Does the same pattern occur? 


```{r Ex6}
library(caret)
library(gbm)

set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"

GBM_sim <- train(y ~ ., data=simulated, method="gbm", verbose=F)
varImp(GBM_sim)

simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1

GBM_sim_2 <- train(y ~ ., data=simulated, method="gbm", verbose=F)
varImp(GBM_sim_2)

simulated$duplicate2 <- simulated$V1 + rnorm(200) * .1

GBM_sim_3 <- train(y ~ ., data=simulated, method="gbm", verbose=F)
varImp(GBM_sim_3)

```  

We can see that a similar pattern appears. The importance of V1 decreases when the first highly correlated predictor is added, and decreases further when the second highly correlated variable is added; both highly correlated variables have similar importance in the model.
  
### Pulling Punches Part 2

The two `.Rdata` files under week 7 come as an abbreviated version of punch profiles from a boxing system to measure acceleration of boxers during live fights. The `gridded` list from the second file below has each individual punch profile which has a list item being a 3 column data frame of time (in ms around the middle of the punch event), acceleration in x (forward-back in g's), and acceleration in y (side-to-side in g's). Also attached are some other fields which are of less importance and contribute to this being a somewhat messy data set.

```{r two}
load(file = 'boxer_features_and_force.Rdata')
load(file = 'force_punch_profiles.Rdata')
```

There are 1000 punch profiles each with an associated force (in Newtons) and boxer who threw the punch. Use the `ff` data frame as ground truth for punch force (variable =`force`) in addition to the rest of the boxer information. Other boxer information included variables around their size to be a proxy for 'effective mass'.

### 7
**Estimations**: Use features (and/or new features) created from your problem set 2 to estimate punch force using a MARS model. Use RMSE as your error estimate.

```{r Ex7}
dat <- gridded

xmean <- c()
xmedian <- c()
xquantile25 <- c()
xquantile75 <- c()
xmax <- c()
xmin <- c()
xdif <- c()

xslope <- c()

ymean <- c()
ymedian <- c()
yquantile25 <- c()
yquantile75 <- c()
ymax <- c()
ymin <- c()
ydif <- c()

meandist <- c()

post_punch_xmin <- c()
pre_punch_xmin <- c()

time_to_hit <- c()
total_ptime <- c()


for (i in 1:length(dat)) {
  time <- dat[[i]]$profile[,1]
  x <- dat[[i]]$profile[,2]
  y <- dat[[i]]$profile[,3]

  xmean[i] <- mean(x)
  xmedian[i] <- median(x)
  xquantile25[i] <- quantile(x)[2]
  xquantile75[i] <- quantile(x)[4]
  xmax[i] <- max(x)
  xmin[i] <- min(x)
  xdif[i] <- max(x) - min(x)
  
  time_p <- time[which(abs(time) <= 200)]
  x_p <- x[which(abs(time) <= 200)]
  y_p <- y[which(abs(time) <= 200)]
  
  time_max <- time_p[which(x_p == max(x_p))][1]
  time_min <- time_p[which(x_p == min(x_p))][1]


  ymean[i] <- mean(y)
  ymedian[i] <- median(y)
  yquantile25[i] <- quantile(y)[2]
  yquantile75[i] <- quantile(y)[4]
  ymax[i] <- max(y)
  ymin[i] <- min(y)
  ydif[i] <- max(y) - min(y)
  
  meandist[i] <- mean(sqrt(x^2 + y^2))
  
  pre_punch_xmin[i] <- min(x_p[which(time_p < 0)])[1]
  pre_punch_time <- time_p[which(x_p == pre_punch_xmin[i])[1]]
  

  post_punch_xmin[i] <- min(x_p[which(time_p > 0)])[1]
  post_punch_time <- time_p[which(x_p == post_punch_xmin[i])[1]]
  
  xslope[i] <- (max(x_p) - pre_punch_xmin[i]) / abs(pre_punch_time - time_max)

  
  time_to_hit[i] <- abs(time_min - time_max)
  total_ptime[i] <- (post_punch_time - time_max) + time_to_hit[i]

  
}


df <- cbind(xmean, xmedian, xquantile25, xquantile75, xmax, xmin, xdif, xslope,
      ymean, ymedian, yquantile25, yquantile75, ymax, ymin, ydif,
      meandist, pre_punch_xmin, post_punch_xmin, time_to_hit, total_ptime)


punch <- cbind(ff, df)

library(earth)

MARS_1 <- earth(force ~ ., data=punch)
preds_mars <- predict(MARS_1, punch)
RMSE(preds_mars, punch$force)


```

The RMSE for the MARS model is 502.37.


### 8
**Estimations improved** Now try a few different (gbm, randomForest) models through the `caret` package and different data splitting techniques to compare. Comparing RMSE which model performs the best?


```{r Ex8}

punch$bag.weight <- NULL
punch$tests <- NULL
punch$boxer <- NULL

set.seed(1)
N <- nrow(punch)
id_test <- sample(1:N, 0.25*N)
ptrain <- punch[-id_test,]
ptest <- punch[id_test,]


# GBM

#10 fold cross validation
set.seed(1)
GBM_10CV <- train(force ~ .,
                     data = ptrain,
                     method = "gbm",
                     metric = "RMSE",
                     trControl = trainControl(method = "cv", number = 10), verbose=F)
preds_GBM_10CV <- predict(GBM_10CV, ptest)
print(paste("RMSE for GBM with 10-fold cross validation:", round(RMSE(preds_GBM_10CV, ptest$force), digits=2)))

#repeated training/test set splitting with 80% training set
set.seed(1)
GBM_80split <- train(force ~ .,
                    data = ptrain,
                    method = "gbm",
                    metric = "RMSE",
                    trControl = trainControl(method = "LGOCV", 
                                             number = 50, 
                                             p = .8), verbose=F)
preds_GBM_80split <- predict(GBM_80split, ptest)
print(paste("RMSE for GBM with training/test set split 80/20:", round(RMSE(preds_GBM_80split, ptest$force), digits=2)))

#bootstrapping
set.seed(1)
GBM_bootstrap <- train(force ~ .,
                    data = ptrain,
                    method = "gbm",
                    metric = "RMSE",
                    trControl = trainControl(method = "boot", number = 50), verbose=F)
preds_GBM_bootstrap <- predict(GBM_bootstrap, ptest)
print(paste("RMSE for GBM with bootstrapping:", round(RMSE(preds_GBM_bootstrap, ptest$force), digits=2)))

#632 bootstrap method
set.seed(1)
GBM_bs632 <- train(force ~ .,
                       data = ptrain,
                       method = "gbm",
                       metric = "RMSE",
                       trControl = trainControl(method = "boot632", 
                                                number = 50), verbose=F)
preds_GBM_bs632 <- predict(GBM_bs632, ptest)
print(paste("RMSE for GBM with 632 bootstrapping method:", round(RMSE(preds_GBM_bs632, ptest$force), digits=2)))

# rf

#10 fold cross validation
set.seed(1)
rf_10CV <- train(force ~ .,
                     data = ptrain,
                     method = "rf",
                     metric = "RMSE",
                     trControl = trainControl(method = "cv", number = 10))
preds_rf_10CV <- predict(rf_10CV, ptest)
print(paste("RMSE for RF with 10-fold cross validation:", round(RMSE(preds_rf_10CV, ptest$force), digits=2)))

#repeated training/test set splitting with 80% training set
set.seed(1)
rf_80split <- train(force ~ .,
                    data = ptrain,
                    method = "rf",
                    metric = "RMSE",
                    trControl = trainControl(method = "LGOCV", 
                                             number = 50, 
                                             p = .8))
preds_rf_80split <- predict(rf_80split, ptest)
print(paste("RMSE for RF with training/test set split 80/20:", round(RMSE(preds_rf_80split, ptest$force), digits=2)))

#bootstrapping
set.seed(1)
rf_bootstrap <- train(force ~ .,
                    data = ptrain,
                    method = "rf",
                    metric = "RMSE",
                    trControl = trainControl(method = "boot", number = 50))
preds_rf_bootstrap <- predict(rf_bootstrap, ptest)
print(paste("RMSE for RF with bootstrapping:", round(RMSE(preds_rf_bootstrap, ptest$force), digits=2)))

#632 bootstrap method
set.seed(1)
rf_bs632 <- train(force ~ .,
                       data = ptrain,
                       method = "rf",
                       metric = "RMSE",
                       trControl = trainControl(method = "boot632", 
                                                number = 50))
preds_rf_bs632 <- predict(rf_bs632, ptest)
print(paste("RMSE for RF with 632 bootstrapping method:", round(RMSE(preds_rf_bs632, ptest$force), digits=2)))

```





